data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
#write.csv(region_names, 'region_id.csv')
data_period <- c('2019-05-23', '2019-11-23')
mm <- best_matches(data = data,
id_variable = "Region",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$region_name)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
mm <- merge(x=mm$BestMatches, y=Region, by.x='Region', by.y='data$region_ID', all.x=TRUE)
write.csv(mm, 'MM_results.csv')
setwd("C:/Users/charles.tan/Documents/GitHub/Essence_stuff/Q120 YTP") #set working directory
###
#import libaries
libs <- c("zoo",
"dtw",
"MarketMatching",
"Rcpp",
"CausalImpact",
"lubridate",
"tidyverse",
"ggplot2",
"reshape2",
"plyr",
"pivottabler",
"dplyr",
"GeoexperimentsResearch")
for (lib in libs) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib)
require(lib, character.only = TRUE)
}
}
#if installing GeoexperimentsResearch fails, run the code below
#install.packages("githubinstall")
#library(githubinstall)
#githubinstall("GeoexperimentsResearch")
#library(GeoexperimentsResearch)
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$date <- as.Date(data_raw$date, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
data_raw$date <- as.Date(data_raw$date, format="%Y-%m-%d")
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$Day <- as.Date(data_raw$date, format="%Y-%m-%d")
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$Day <- as.Date(data_raw$Day, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
data_period <- c('2019-05-23', '2019-11-23')
mm <- best_matches(data = data,
id_variable = "Region",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$region_name)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
mm <- merge(x=mm$BestMatches, y=Region, by.x='Region', by.y='data$region_ID', all.x=TRUE)
head(mm)
head(mm$BestMatches)
mm <- best_matches(data = data,
id_variable = "Region",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$region_name)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
head(data)
mm <- best_matches(data = data,
id_variable = "Region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$region_name)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(data)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$region_name)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
region_names
data_raw$Day <- as.Date(data_raw$Day, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('region_ID', 'Region')]
region_names
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('region_ID', 'Region')]
region_names
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
region_names
data_period <- c('2019-05-23', '2019-11-23')
head(data)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='Region', by.y='region_ID', all.x=TRUE)
View(region_names)
View(region_names)
View(mm)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_id', by.y='data$region_ID', all.x=TRUE)
View(mm)
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_ID', by.y='data$region_ID', all.x=TRUE)
head(mm)
write.csv(mm, 'MM_results.csv')
mm <- mm[, c('region_ID', 'BestControl', 'RelativeDistance', 'Correlation', 'rank', 'data$Region')]
head(mm)
colnames(mm)
colnames(mm['data$Region'])
colnames(mm['data$Region']) <- 'Region'
head(mm)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'Region'
head(mm)
mm <- merge(x=mm, y=region_names, by.x='BestControl', by.y='data$region_ID', all.x=TRUE)
head(mm)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'ComparedRegion'
mm <- mm[order('Region', 'rank')]
head(mm)
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$Day <- as.Date(data_raw$Day, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
region_names
#write.csv(region_names, 'region_id.csv')
data_period <- c('2019-05-23', '2019-11-23')
head(data)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_ID', by.y='data$region_ID', all.x=TRUE)
mm <- mm[, c('region_ID', 'BestControl', 'RelativeDistance', 'Correlation', 'rank', 'data$Region')]
colnames(mm)[colnames(mm) == 'data$Region'] <- 'Region'
mm <- merge(x=mm, y=region_names, by.x='BestControl', by.y='data$region_ID', all.x=TRUE)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'ComparedRegion'
mm <- mm[order('Region', 'rank')]
head(mm)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_ID', by.y='data$region_ID', all.x=TRUE)
mm <- mm[, c('region_ID', 'BestControl', 'RelativeDistance', 'Correlation', 'rank', 'data$Region')]
colnames(mm)[colnames(mm) == 'data$Region'] <- 'Region'
mm <- merge(x=mm, y=region_names, by.x='BestControl', by.y='data$region_ID', all.x=TRUE)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'ComparedRegion'
head(mm)
mm <- mm[order('Region', 'rank'),]
head(mm)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_ID', by.y='data$region_ID', all.x=TRUE)
mm <- mm[, c('region_ID', 'BestControl', 'RelativeDistance', 'Correlation', 'rank', 'data$Region')]
colnames(mm)[colnames(mm) == 'data$Region'] <- 'Region'
mm <- merge(x=mm, y=region_names, by.x='BestControl', by.y='data$region_ID', all.x=TRUE)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'ComparedRegion'
head(mm)
mm <- mm[order(mm$Region, mm$rank),]
head(mm)
write.csv(mm, 'MM_results.csv')
write.csv(mm, 'MM_results.csv', row.names=FALSE)
setwd("C:/Users/charles.tan/Documents/GitHub/Essence_stuff/Q120 YTP") #set working directory
###
#import libaries
libs <- c("zoo",
"dtw",
"MarketMatching",
"Rcpp",
"CausalImpact",
"lubridate",
"tidyverse",
"ggplot2",
"reshape2",
"plyr",
"pivottabler",
"dplyr",
"GeoexperimentsResearch")
for (lib in libs) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib)
require(lib, character.only = TRUE)
}
}
#if installing GeoexperimentsResearch fails, run the code below
#install.packages("githubinstall")
#library(githubinstall)
#githubinstall("GeoexperimentsResearch")
#library(GeoexperimentsResearch)
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$Day <- as.Date(data_raw$Day, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
region_names
#write.csv(region_names, 'region_id.csv')
data_period <- c('2018-11-23', '2019-11-23')
head(data)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_ID', by.y='data$region_ID', all.x=TRUE)
mm <- mm[, c('region_ID', 'BestControl', 'RelativeDistance', 'Correlation', 'rank', 'data$Region')]
colnames(mm)[colnames(mm) == 'data$Region'] <- 'Region'
mm <- merge(x=mm, y=region_names, by.x='BestControl', by.y='data$region_ID', all.x=TRUE)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'ComparedRegion'
head(mm)
mm <- mm[order(mm$Region, mm$rank),]
head(mm)
write.csv(mm, 'MM_results.csv', row.names=FALSE)
##user input
setwd("C:/Users/charles.tan/Documents/GitHub/Essence_stuff/Q120 YTP") #set working directory
###
#import libaries
libs <- c("zoo",
"dtw",
"MarketMatching",
"Rcpp",
"CausalImpact",
"lubridate",
"tidyverse",
"ggplot2",
"reshape2",
"plyr",
"pivottabler",
"dplyr",
"GeoexperimentsResearch")
for (lib in libs) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib)
require(lib, character.only = TRUE)
}
}
#if installing GeoexperimentsResearch fails, run the code below
#install.packages("githubinstall")
#library(githubinstall)
#githubinstall("GeoexperimentsResearch")
#library(GeoexperimentsResearch)
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$Day <- as.Date(data_raw$Day, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
region_names
#write.csv(region_names, 'region_id.csv')
data_period <- c('2018-11-23', '2019-11-23')
head(data)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
###user input
setwd("C:/Users/charles.tan/Documents/GitHub/Essence_stuff/Q120 YTP") #set working directory
###
#import libaries
libs <- c("zoo",
"dtw",
"MarketMatching",
"Rcpp",
"CausalImpact",
"lubridate",
"tidyverse",
"ggplot2",
"reshape2",
"plyr",
"pivottabler",
"dplyr",
"GeoexperimentsResearch")
for (lib in libs) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib)
require(lib, character.only = TRUE)
}
}
#if installing GeoexperimentsResearch fails, run the code below
#install.packages("githubinstall")
#library(githubinstall)
#githubinstall("GeoexperimentsResearch")
#library(GeoexperimentsResearch)
#read in dataset
ConversionsRaw <- read.csv("signups.csv",header=T)
ConversionsRaw <- ConversionsRaw[ , c("Day", "Region", "Signups")]
head(ConversionsRaw)
ConversionsRaw$Day<-as.Date(ConversionsRaw$Day, format="%Y-%m-%d")
ConversionsData <- ConversionsRaw[complete.cases(ConversionsRaw), ]
ConversionsData<-ConversionsData[order(ConversionsData$Region),]
ConversionsData$CountryID <- cumsum(!duplicated(ConversionsData$Region))
region_names<-ddply(ConversionsData, .(ConversionsData$CountryID, ConversionsData$Region), nrow)
region_names<-region_names[-3]
names(region_names)<-c("CountryID","Region")
region_names
#write.csv(region_names,"region_names.csv")
############################################
#This section will return us the RelativeDistance and Correlation computation
#as well as BestControl markets for each region.
############################################
###user input
data_period = c("2018-11-23", "2019-11-23") #select period of data to focus
###
#output dtw relative distance and correlation scores for all regions
mm <- best_matches(data = ConversionsData,
id_variable = "CountryID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(ConversionsData$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
###user input
RD_cutoff = 30 #set cutoff percentile for ave relative distance
corr_cutoff = 0.5 #set cutoff value for ave correlation of population
n = 13 #set desired number of exposed markets
###
length(mm$BestMatches)
mm$BestMatches['corr_tag_90']<-ifelse(mm$BestMatches$Correlation >=0.9, 1,0)
mm$BestMatches['corr_tag_80']<-ifelse(mm$BestMatches$Correlation >=0.8, 1,0)
mm$BestMatches = mutate(mm$BestMatches, percentile_rank = ntile(mm$BestMatches$RelativeDistance,100))
meandf<-aggregate(mm$BestMatches[,c('Correlation','RelativeDistance')], list(mm$BestMatches$CountryID), mean)
head(meandf)
meandf = mutate(meandf, percentile_rank_RD = ntile(meandf$RelativeDistance,100))
meandf = mutate(meandf, percentile_rank_Corr = ntile(meandf$Correlation,100))
meandf['pop_ave_dtw_topP']<-ifelse(meandf$percentile_rank_RD < RD_cutoff, 1,0)
meandf['pop_ave_corr_more']<-ifelse(meandf$Correlation > corr_cutoff, 1,0)
length(meandf)
colnames(meandf)[colnames(meandf)=="Group.1"]<-"CountryID"
newdf <- meandf[,c(1,6,7)]
MatchScores2 <- merge(mm$BestMatches, newdf, by="CountryID")
head(MatchScores2)
colnames(MatchScores2)
write.csv(MatchScores2,"YTPJPQ120_MatchScores.csv")
MatchScores3<-MatchScores2[MatchScores2$corr_tag_90==1 & MatchScores2$pop_ave_corr_more==1 & MatchScores2$pop_ave_dtw_topP==1,]
head(MatchScores3)
nrow(MatchScores3)
summary(MatchScores3)
ChosenMarkets<-data.frame(table(MatchScores3$CountryID))
names(ChosenMarkets)[1]<-"CountryID"
ChosenMarkets<-merge(ChosenMarkets,region_names,by="CountryID")
head(ChosenMarkets)
exposed <- ChosenMarkets[rev(order(ChosenMarkets$Freq)),]
exposed2 <- exposed[1:n,1]
exposed_data <- MatchScores3[MatchScores3$CountryID %in% exposed2 ,]
exposed_data
control_mix<-data.frame(table(exposed_data$BestControl))
names(region_names)[1]<-"Var1"
control_mix<-merge(control_mix,region_names,by="Var1")
exposed[1:n,] #list n possible exposed markets with the highest number of potential control pairs
control_mix[control_mix$Freq==n,] #list control markets common across exposed
###user input
setwd("C:/Users/charles.tan/Documents/GitHub/Essence_stuff/Q120 YTP") #set working directory
###
#import libaries
libs <- c("zoo",
"dtw",
"MarketMatching",
"Rcpp",
"CausalImpact",
"lubridate",
"tidyverse",
"ggplot2",
"reshape2",
"plyr",
"pivottabler",
"dplyr",
"GeoexperimentsResearch")
for (lib in libs) {
if (!require(lib, character.only = TRUE)) {
install.packages(lib)
require(lib, character.only = TRUE)
}
}
#if installing GeoexperimentsResearch fails, run the code below
#install.packages("githubinstall")
#library(githubinstall)
#githubinstall("GeoexperimentsResearch")
#library(GeoexperimentsResearch)
data_raw = read.csv('signups.csv', header=T, encoding='UTF-8')
colnames(data_raw)[1] <- 'Day'
print(head(data_raw))
data_raw$Day <- as.Date(data_raw$Day, format="%Y-%m-%d")
data <- data_raw[complete.cases(data_raw), ]
data <- data[order(data$Region), ]
data$region_ID <- cumsum(!duplicated(data$Region))
region_names <- ddply(data, .(data$region_ID, data$Region), nrow)
region_names <- region_names[, c('data$region_ID', 'data$Region')]
region_names
#write.csv(region_names, 'region_id.csv')
data_period <- c('2018-11-23', '2019-11-23')
head(data)
mm <- best_matches(data = data,
id_variable = "region_ID",
date_variable = "Day",
matching_variable = "Signups",
parallel = TRUE,
warping_limit = 1,
dtw_emphasis = 1, #setting dtw_emphasis to 1 ensures scores are output- no impact to chosen control
matches = length(unique(data$Region)), # retrieve scores for all markets for each market
start_match_period = data_period[1],
end_match_period = data_period[2])
head(mm$BestMatches)
mm <- merge(x=mm$BestMatches, y=region_names, by.x='region_ID', by.y='data$region_ID', all.x=TRUE)
mm <- mm[, c('region_ID', 'BestControl', 'RelativeDistance', 'Correlation', 'rank', 'data$Region')]
colnames(mm)[colnames(mm) == 'data$Region'] <- 'Region'
mm <- merge(x=mm, y=region_names, by.x='BestControl', by.y='data$region_ID', all.x=TRUE)
colnames(mm)[colnames(mm) == 'data$Region'] <- 'ComparedRegion'
head(mm)
mm <- mm[order(mm$Region, mm$rank),]
head(mm)
write.csv(mm, 'MM_results.csv', row.names=FALSE)
